one <- print(citation(), style = "textVersion")  # Citation for R
cite.version <- R.Version()
pip <- as.character(cite.version$version.string)
cat("Version", pip, "\n")
# List of specific R packages used in the program
specific_packages <- c(
"car",            # For regression analysis and diagnostic tools
"caret",          # For training machine learning models and evaluating performance
"rpart",          # For recursive partitioning and decision tree models
"rpart.plot",     # For visualizing decision trees created by rpart
"forecast",       # For time series forecasting
"DataExplorer",   # For exploratory data analysis (EDA)
"ggplot2",        # For advanced data visualization
"formattable",    # For creating interactive tables
"knitr",          # For dynamic report generation
"dlookr",          # For data exploration and diagnostic tools
"corrplot",       # For visualizing correlation matrices
"summarytools"    # For summarizing and profiling datasets
)
for (i in seq_along(specific_packages)) {
citation_text <- capture.output(print(citation(specific_packages[i]), style = "textVersion"))
wrapped_text <- strwrap(citation_text, width = 80, simplify = TRUE)
cat(wrapped_text, sep = "\n")
if (i < length(specific_packages)) {
cat("\n")  # Add a single line break between references
}
}
# Displaying the new values
kable(head(credit, 10))
# Prepare data for Logistic Regression (no scaling, including all variables)
credit_LR <- credit %>%
dplyr::select(CreditRisk, everything())  # Select all variables, no scaling yet
library(rpart)
library(rpart.plot)
set.seed(1)
# Train full classification tree model
full_tree <- rpart(CreditRisk ~ ., data = train_cart, method = "class", cp = 0, minsplit = 2, minbucket = 1)
cat("\nVariable Importance\n")
print(full_tree$variable.importance)
# Plot the full tree
prp(full_tree, type = 1, extra = 1, under = TRUE)
printcp(full_tree)
options(scipen=999)
suppressWarnings(RNGversion("3.5.3"))
library(tidyverse)
library(DataExplorer)
library(flextable)
library(gridExtra)
library(caret)
library(gains)
library(pROC)
library(klaR)
library(rpart)
library(rpart.plot)
credit <- read.table("credit/german.data", header = TRUE, sep = " ")
str(credit)
head(credit,15)
credit %>% head(3)
credit %>% tail(3)
credit %>% plot_intro()
psych::describe(credit, fast = TRUE)
## Variable X1.1 = Credit Risk- Handled in the next section
credit$X1.1 <- as.factor(credit$X1.1)
credit%>% plot_histogram()
credit %>% plot_boxplot(by = "X1.1")
library(dlookr)
dlookr::diagnose_outlier(credit)
boxplot(credit$X1.1, plot = FALSE)$out
summary(credit$X1.1)
# Rename columns with meaningful names
colnames(credit) <- c(
"CheckingAccountStatus",     # A11
"LoanDuration",              # X6
"CreditHistory",             # A34
"LoanPurpose",               # A43
"CreditAmount",              # X1169
"SavingsAccountBonds",       # A65
"EmploymentDuration",        # A75
"InstallmentRate",           # X4
"PersonalStatusAndSex",      # A93
"OtherDebtorsGuarantors",    # A101
"CurrentResidenceDuration",  # X4.1
"Property",                  # A121
"CustomerAge",               # X67
"OtherInstallmentPlans",     # A143
"HousingStatus",             # A152
"ExistingCredits",           # X2
"JobCategory",               # A173
"Dependents",                # X1
"HasPhone",                  # A192
"ForeignWorker",             # A201
"CreditRisk"                 # Target variable (Good/Bad classification)
)
# Recode categorical values with meaningful labels
credit$CheckingAccountStatus <- factor(credit$CheckingAccountStatus,
levels = c("A11", "A12", "A13", "A14"),
labels = c("< 0 DM", "0-200 DM", ">= 200 DM", "No Account"))
credit$CreditHistory <- factor(credit$CreditHistory,
levels = c("A30", "A31", "A32", "A33", "A34"),
labels = c("No credits", "All paid", "Paid till now",
"Delayed payment", "Critical/Other"))
credit$LoanPurpose <- factor(credit$LoanPurpose,
levels = c("A40", "A41", "A42", "A43", "A44",
"A45", "A46", "A47", "A48", "A49", "A410"),
labels = c("New Car", "Used Car", "Furniture",
"Radio/TV", "Appliances", "Repairs",
"Education", "Vacation", "Retraining",
"Business", "Others"))
credit$SavingsAccountBonds <- factor(credit$SavingsAccountBonds,
levels = c("A61", "A62", "A63", "A64", "A65"),
labels = c("< 100 DM", "100-500 DM",
"500-1000 DM", ">= 1000 DM", "No Savings"))
credit$EmploymentDuration <- factor(credit$EmploymentDuration,
levels = c("A71", "A72", "A73", "A74", "A75"),
labels = c("Unemployed", "< 1 year",
"1-4 years", "4-7 years", ">= 7 years"))
credit$PersonalStatusAndSex <- factor(credit$PersonalStatusAndSex,
levels = c("A91", "A92", "A93", "A94", "A95"),
labels = c("Male Divorced", "Female Divorced/Married",
"Male Single", "Male Married/Widowed",
"Female Single"))
credit$OtherDebtorsGuarantors <- factor(credit$OtherDebtorsGuarantors,
levels = c("A101", "A102", "A103"),
labels = c("None", "Co-applicant", "Guarantor"))
credit$Property <- factor(credit$Property,
levels = c("A121", "A122", "A123", "A124"),
labels = c("Real Estate", "Savings/Life Insurance",
"Car/Other", "No Property"))
credit$OtherInstallmentPlans <- factor(credit$OtherInstallmentPlans,
levels = c("A141", "A142", "A143"),
labels = c("Bank", "Stores", "None"))
credit$HousingStatus <- factor(credit$HousingStatus,
levels = c("A151", "A152", "A153"),
labels = c("Rent", "Own", "Free"))
credit$JobCategory <- factor(credit$JobCategory,
levels = c("A171", "A172", "A173", "A174"),
labels = c("Unskilled-Non-resident", "Unskilled-Resident",
"Skilled Employee", "Management/High Qualified"))
credit$HasPhone <- factor(credit$HasPhone,
levels = c("A191", "A192"),
labels = c("No", "Yes"))
credit$ForeignWorker <- factor(credit$ForeignWorker,
levels = c("A201", "A202"),
labels = c("Yes", "No"))
# Displaying the new values
head(credit)
# Convert CreditRisk to 0 and 1
credit$CreditRisk <- as.factor(ifelse(credit$CreditRisk == 1, 0, 1))
# Verify the change
table(credit$CreditRisk)
# Prepare data for Logistic Regression (no scaling, including all variables)
credit_LR <- credit %>%
dplyr::select(CreditRisk, everything())  # Select all variables, no scaling yet
# Partition 60/40 and check proportions
library(caret)
set.seed(1)
myIndex_LR <- createDataPartition(credit_LR$CreditRisk, p = 0.6, list = FALSE)
trainSet_LR <- credit_LR[myIndex_LR, ]
testSet_LR  <- credit_LR[-myIndex_LR, ]
# Check proportions
cat("Full Dataset"); prop.table(table(credit_LR$CreditRisk))
cat("\nTrain Dataset"); prop.table(table(trainSet_LR$CreditRisk))
cat("\nTest Dataset");  prop.table(table(testSet_LR$CreditRisk))
# Logistic Regression model without scaling and using all variables
library(glmnet)
# Train Logistic Regression model
LR_fit <- glm(CreditRisk ~ ., data = trainSet_LR, family = binomial)
summary(LR_fit)
# Exponentiate coefficients and display as odds ratios
odds_ratios <- exp(coef(LR_fit))
# Create a data frame for better readability
odds_ratios_df <- data.frame(
Variables = names(odds_ratios),
Odds_Ratios = odds_ratios
)
# Print the result
print(odds_ratios_df)
# Fit Metrics
library(broom)
# Tidy summary of the model for metrics
tidy_LR_fit <- tidy(LR_fit)
print(tidy_LR_fit)
# Predict class using the logistic regression model
predicted_class_LR <- ifelse(predict(LR_fit, newdata = testSet_LR, type = "response") > 0.5, 1, 0)
# Create confusion matrix
CM_LR <- caret::confusionMatrix(as.factor(predicted_class_LR), testSet_LR$CreditRisk, positive = "1")
CM_LR
precision_LR <- unname(CM_LR$byClass['Pos Pred Value'])
recall_LR    <- unname(CM_LR$byClass['Sensitivity'])
f1_score_LR <- 2 * ((precision_LR * recall_LR) / (precision_LR + recall_LR))
cat("F1 Score: ", f1_score_LR, "\n")
predicted_prob_LR <- predict(LR_fit, newdata = testSet_LR, type = "response")
roc_curve_LR <- pROC::roc(as.numeric(testSet_LR$CreditRisk), predicted_prob_LR)
optimal_cutoff_LR <- coords(roc_curve_LR, "best", ret = "threshold")
cat("Optimal Cutoff: ", optimal_cutoff_LR$threshold, "\n")
predicted_class_opt_LR <- ifelse(predicted_prob_LR >= optimal_cutoff_LR$threshold, 1, 0)
cat("CONFUSION MATRIX AT OPTIMAL CUTOFF VALUE OF:", optimal_cutoff_LR$threshold, "\n\n")
CM_opt_LR <- confusionMatrix(as.factor(predicted_class_opt_LR), as.factor(testSet_LR$CreditRisk), positive = '1')
CM_opt_LR
precision_opt_LR <- unname(CM_opt_LR$byClass['Pos Pred Value'])
recall_opt_LR    <- unname(CM_opt_LR$byClass['Sensitivity'])
f1_score_LR_opt <- 2 * ((precision_opt_LR * recall_opt_LR) / (precision_opt_LR + recall_opt_LR))
cat("F1 Score at Optimal Cutoff: ", f1_score_LR_opt, "\n")
testSet_LR$CreditRisk <- as.numeric(as.character(testSet_LR$CreditRisk))
gains_table_LR <- gains(testSet_LR$CreditRisk, predicted_prob_LR)
gains_plot_LR <- ggplot() +
geom_line(aes(x = c(0, gains_table_LR$cume.obs), y = c(0, gains_table_LR$cume.pct.of.total * sum(testSet_LR$CreditRisk))),
color = "blue") +
geom_line(aes(x = c(0, dim(testSet_LR)[1]), y = c(0, sum(testSet_LR$CreditRisk))),
color = "red", linetype = "dashed") +
labs(x = "# of cases", y = "Cumulative", title = "Cumulative Gains Chart for Logistic Regression") +
theme_minimal()
roc_object_LR <- pROC::roc(testSet_LR$CreditRisk, predicted_prob_LR)
auc_LR <- auc(roc_object_LR)
roc_plot_LR <- ggroc(roc_object_LR) +
geom_segment(aes(x = 1, y = 0, xend = 0, yend = 1), linetype = "dashed", color = "red") +
labs(x = "1 - Specificity", y = "Sensitivity", title = "ROC Curve for Logistic Regression") +
annotate("text", x = 0.5, y = 0.05, label = paste("AUC =", round(auc_LR, 4)), size = 5, color = "blue") +
theme_minimal()
grid.arrange(gains_plot_LR, roc_plot_LR, ncol = 2)
# No data should be prepared
set.seed(1)
# Create data partition for CART model
index_cart <- createDataPartition(credit_LR$CreditRisk, p = 0.6, list = FALSE)
train_cart <- credit_LR[index_cart, ]
testSet_CT <- credit_LR[-index_cart, ]
cat("Full Dataset:\n")
print(prop.table(table(credit_LR$CreditRisk)))
cat("\nTrain Dataset:\n")
print(prop.table(table(train_cart$CreditRisk)))
cat("\nTest Dataset:\n")
print(prop.table(table(testSet_CT$CreditRisk)))  # Corrected here
library(rpart)
library(rpart.plot)
set.seed(1)
# Train full classification tree model
full_tree <- rpart(CreditRisk ~ ., data = train_cart, method = "class", cp = 0, minsplit = 2, minbucket = 1)
cat("\nVariable Importance\n")
print(full_tree$variable.importance)
# Plot the full tree
prp(full_tree, type = 1, extra = 1, under = TRUE)
printcp(full_tree)
# Find the row with minimum xerror
min_xerror_row <- which.min(full_tree$cptable[,"xerror"])
# Find the CP value of the first tree whose xerror is within one standard deviation of the minimum
best_row <- min(which(full_tree$cptable[,"xerror"] <=
(full_tree$cptable[min_xerror_row,"xerror"] +
full_tree$cptable[min_xerror_row,"xstd"])))
# Get the CP for this row
best_cp <- full_tree$cptable[best_row, "CP"]
best_cp
rounded_cp <-  round(full_tree$cptable[5,1] + .000001,6)
ME_pruned_tree <- prune(full_tree, cp = rounded_cp)
cat("\nVariable Importance after Pruning\n")
print(ME_pruned_tree$variable.importance)
# Plot the pruned tree
prp(ME_pruned_tree, type = 1, extra = 1, under = TRUE)
gc()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(scipen=999)
suppressWarnings(RNGversion("3.5.3"))
library(car)
library(caret)
library(rpart)
library(rpart.plot)
library(forecast)
library(DataExplorer)
library(ggplot2)
library(formattable)
library(knitr)
library(dlookr)
library(corrplot)
library(summarytools)
bostonData <- read.csv("boston.csv", header = TRUE)
head(bostonData, 10)
library(DataExplorer)
plot_intro(bostonData)
plot_missing(bostonData)
psych::describe(bostonData, fast = TRUE)
# continuous variables -- descriptives, histograms and Y~X plots
summarytools::descr(bostonData)
plot_histogram(bostonData)  #continuous
plot_scatterplot(split_columns(bostonData)$continuous, by = "MEDV", sampled_rows = 1000L) #Y~X plots
# look at variable correlations
#  -- here are 2 different plots types -- you may find one easier to use
## version 1 in one line of code
plot_correlation(bostonData)
library(ggplot2)
# Quadratic relationship: Age vs. Median Home Value (MEDV)
ggplot(bostonData, aes(x = AGE, y = MEDV)) +
geom_point() +
stat_smooth(method = "lm", formula = y ~ poly(x, 2), se = FALSE, colour = "red") +
labs(
title = "Relationship between Median Home Value and Age",
subtitle = "With Quadratic Regression Line",
x = "Proportion of Owner-Occupied Units Built Before 1940",
y = "Median Home Value ($1000s)"
)
# Exponential relationship: Distance to Employment Centers (DIS) vs. Median Home Value (MEDV)
ggplot(bostonData, aes(x = DIS, y = MEDV)) +
geom_point() +
geom_smooth(
method = "glm",
method.args = list(family = gaussian(link = "log")),
se = FALSE
) +
labs(
title = "Relationship between Median Home Value and Distance to Employment Centers",
subtitle = "With Exponential Trendline",
x = "Distance to Employment Centers",
y = "Median Home Value ($1000s)"
)
library(ggplot2)
# Create interaction plot
ggplot(bostonData, aes(x = RM, y = MEDV, color = cut(LSTAT, breaks = 3))) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Interaction Effect of RM and LSTAT on MEDV",
x = "Average Number of Rooms (RM)",
y = "Median Home Value (MEDV, $1000s)",
color = "LSTAT Level")
ggplot(bostonData, aes(x = DIS, y = MEDV, color = factor(RAD))) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Interaction Effect of DIS and RAD on MEDV",
x = "Distance to Employment Centers (DIS)",
y = "Median Home Value (MEDV, $1000s)",
color = "Highway Accessibility (RAD)")
bostonData$RAD_group <- cut(bostonData$RAD, breaks = c(0, 4, 8, 24), labels = c("Low", "Medium", "High"))
ggplot(bostonData, aes(x = DIS, y = MEDV, color = RAD_group)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Interaction Effect of DIS and RAD on MEDV",
x = "Distance to Employment Centers (DIS)",
y = "Median Home Value (MEDV, $1000s)",
color = "Highway Accessibility (Grouped)")
ggplot(bostonData, aes(x = TAX, y = MEDV, color = cut(NOX, breaks = 3))) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Interaction Effect of TAX and NOX on MEDV",
x = "Property Tax Rate (TAX)",
y = "Median Home Value (MEDV, $1000s)",
color = "Nitric Oxide Levels (NOX)")
ggplot(bostonData, aes(x = PTRATIO, y = MEDV, color = cut(AGE, breaks = 3))) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title = "Interaction Effect of PTRATIO and AGE on MEDV",
x = "Pupil-Teacher Ratio (PTRATIO)",
y = "Median Home Value (MEDV, $1000s)",
color = "Proportion of Older Homes (AGE)")
# Apply log transformation to CRIM to reduce skewness and retain variability
bostonData$CRIM <- log1p(bostonData$CRIM)  # log1p handles zero values
set.seed(1)
myIndex <- createDataPartition(bostonData$MEDV , p=0.7, list=FALSE)
trainSet <- bostonData[myIndex,]
validationSet <- bostonData[-myIndex,]
cat("PARTITION:  SUMMARY OF FULL DATASET -- ALL VARIABLES\n")
formattable::percent(prop.table(table(bostonData$MEDV )))
summary(bostonData)
cat("\nPARTITION:  SUMMARY OF TRAIN DATASET -- ALL VARIABLES\n");
formattable::percent(prop.table(table(trainSet$MEDV )))
summary(trainSet)
cat("\nPARTITION:  SUMMARY OF VALIDATION DATASET -- ALL VARIABLES\n");
formattable::percent(prop.table(table(validationSet$MEDV )))
summary(validationSet)
model1 <- lm(MEDV ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT, data = trainSet)
summary(model1)
vif(model1)
model2 <- lm(MEDV ~ NOX * DIS + RM * LSTAT + INDUS * RAD + TAX * PTRATIO + ZN * CHAS, data = trainSet)
summary(model2)
model3 <- lm(MEDV ~ CRIM + ZN + INDUS + CHAS + NOX +
poly(AGE, 2, raw=TRUE) + DIS + RAD + log(TAX) +
PTRATIO + log(CRIM) + B + LSTAT +
RM * TAX, data=trainSet)
summary(model3)
model4 <- lm(
MEDV ~ poly(AGE, 2, raw = TRUE) + log(CRIM) + ZN + INDUS + CHAS +
NOX * DIS + RM * LSTAT + INDUS * RAD + log(TAX) + PTRATIO +
B + LSTAT,
data = trainSet
)
summary(model4)
set.seed(1)
default_tree <- rpart(MEDV  ~ ., data = trainSet, method = "anova")
#summary(default_tree)
cat("DEFAULT TREE\n")
prp(default_tree, type = 1, extra = 1, under = TRUE)
set.seed(1)
full_tree <- rpart(MEDV ~ ., data = trainSet, method = "anova", cp = 0, minsplit = 2, minbucket = 1)
# Visualize the full tree
prp(full_tree, type = 1, extra = 1, under = TRUE)
printcp(full_tree)
pip <- printcp(full_tree)
cat("FULL TREE - Identify the complexity parameter (cp) associated with the smallest cross-validated prediction error\n")
head(pip,10)
# Prune the tree using the optimal cp
pruned_tree <- prune(full_tree, cp = 0.012935755)
cat("PRUNED TREE\n")
prp(pruned_tree, type = 1, extra = 1, under = TRUE)
predict1 <- predict(model1, newdata=validationSet)
performance1 <- data.frame(forecast::accuracy(predict1, validationSet$MEDV))
RMSE1 <- performance1$RMSE
MAPE1 <- performance1$MAPE
RMSE1; MAPE1
predict2 <- predict(model2, newdata=validationSet)
performance2 <- data.frame(forecast::accuracy(predict2, validationSet$MEDV))
RMSE2 <- performance2$RMSE
MAPE2 <- performance2$MAPE
RMSE2; MAPE2
predict3 <- predict(model3, newdata=validationSet)
performance3 <- data.frame(forecast::accuracy(predict3, validationSet$MEDV))
RMSE3 <- performance3$RMSE
MAPE3 <- performance3$MAPE
RMSE3; MAPE3
predict4 <- predict(model4, newdata=validationSet)
performance4 <- data.frame(forecast::accuracy(predict4, validationSet$MEDV))
RMSE4 <- performance4$RMSE
MAPE4 <- performance4$MAPE
RMSE4; MAPE4
predict5 <- predict(pruned_tree, validationSet)
performance5 <- data.frame(forecast::accuracy(predict5, validationSet$MEDV))
RMSE5 <- performance5$RMSE
MAPE5 <- performance5$MAPE
RMSE5; MAPE5
# This is the final model which was selected
model3 <- lm(MEDV ~ CRIM + ZN + INDUS + CHAS + NOX +
poly(AGE, 2, raw=TRUE) + DIS + RAD + log(TAX) +
PTRATIO + log(CRIM) + B + LSTAT +
RM * TAX, data=trainSet)
summary(model3)
coefficients <- coef(model3)  # Extract coefficients
# Extract the coefficients for poly(AGE, 2, raw = TRUE)
linear_coeff <- coefficients["poly(AGE, 2, raw = TRUE)1"]
quadratic_coeff <- coefficients["poly(AGE, 2, raw = TRUE)2"]
# Calculate the optimal point
optimal_AGE <- -linear_coeff / (2 * quadratic_coeff)
cat("The optimal point for AGE is:", optimal_AGE, "\n")
# Citation for R including the version
one <- print(citation(), style = "textVersion")  # Citation for R
cite.version <- R.Version()
pip <- as.character(cite.version$version.string)
cat("Version", pip, "\n")
# List of specific R packages used in the program
specific_packages <- c(
"car",            # For regression analysis and diagnostic tools
"caret",          # For training machine learning models and evaluating performance
"rpart",          # For recursive partitioning and decision tree models
"rpart.plot",     # For visualizing decision trees created by rpart
"forecast",       # For time series forecasting
"DataExplorer",   # For exploratory data analysis (EDA)
"ggplot2",        # For advanced data visualization
"formattable",    # For creating interactive tables
"knitr",          # For dynamic report generation
"dlookr",          # For data exploration and diagnostic tools
"corrplot",       # For visualizing correlation matrices
"summarytools"    # For summarizing and profiling datasets
)
for (i in seq_along(specific_packages)) {
citation_text <- capture.output(print(citation(specific_packages[i]), style = "textVersion"))
wrapped_text <- strwrap(citation_text, width = 80, simplify = TRUE)
cat(wrapped_text, sep = "\n")
if (i < length(specific_packages)) {
cat("\n")  # Add a single line break between references
}
}
@misc{UCI2024,
#Dataset
cat(
"Dataset: Statlog (German Credit Data)\n",
"UCI Machine Learning Repository. Statlog (German Credit Data) Dataset.\n",
"Retrieved from https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
"\n"
)
# Citation for R including the version
one <- print(citation(), style = "textVersion")  # Citation for R
cite.version <- R.Version()
pip <- as.character(cite.version$version.string)
cat("Version", pip, "\n")
# List of specific R packages used in the program
specific_packages <- c(
"car",            # For regression analysis and diagnostic tools
"caret",          # For training machine learning models and evaluating performance
"rpart",          # For recursive partitioning and decision tree models
"rpart.plot",     # For visualizing decision trees created by rpart
"forecast",       # For time series forecasting
"DataExplorer",   # For exploratory data analysis (EDA)
"ggplot2",        # For advanced data visualization
"formattable",    # For creating interactive tables
"knitr",          # For dynamic report generation
"dlookr",          # For data exploration and diagnostic tools
"corrplot",       # For visualizing correlation matrices
"summarytools"    # For summarizing and profiling datasets
)
for (i in seq_along(specific_packages)) {
citation_text <- capture.output(print(citation(specific_packages[i]), style = "textVersion"))
wrapped_text <- strwrap(citation_text, width = 80, simplify = TRUE)
cat(wrapped_text, sep = "\n")
if (i < length(specific_packages)) {
cat("\n")  # Add a single line break between references
}
}
grid.arrange(gains_plot_LR, roc_plot_LR, ncol = 2)
library(shiny); runApp('C:/Users/robin/OneDrive - John Brown University/Spring 2025/Capstone II/SMLPDashboard.R')
runApp('C:/Users/robin/OneDrive - John Brown University/Spring 2025/Capstone II/SMLPDashboard.R')
runApp('C:/Users/robin/OneDrive/Desktop/SMLPD')
setwd("C:/Users/robin/OneDrive/Desktop/SMLPD")
install.packages("renv")
renv::init()
setwd("C:/Users/robin/OneDrive/Desktop/SMLPD")
install.packages("renv")
renv::init()
library(shiny)
